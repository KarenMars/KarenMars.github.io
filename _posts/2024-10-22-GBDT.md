## Gradient Boosted Decision Trees

> I got to know the series of Gradient Boosted Decision Trees (GBDT) at the time for developing classification/regression algorithms for time series data. GBDT algorithms include XGBoost, LightGBM and CatBoost. This article gives a brief explanation of the basic idea of GBDT.

### Useful Resources (References)
- [Clear explanation of GBDT](https://www.youtube.com/watch?v=3CC4N4z3GJc&t=78s)

### Questions
When we come to algorithms based on trees, we would like to know some questions of how to construct trees. Specifically, the trees have leaf nodes and splitting nodes. The questions are listed below:

1. How to choose the proper features and the value of the features for constructing a splitting nodes?
2. When to stop split the node and obtain the leaf node?
3. How to combine the results of different trees or what are the relationships between different trees?
4. How to constrain the size of trees?

In the following sections, these questions will be answered.

### GBDT trees for regression
Let's start with building trees for regression. Given an example of predicting weights from height, favorite color, and gender shown in the figure below:

<img src="https://github.com/KarenMars/KarenMars.github.io/blob/main/images/gbdt/20240729165908.png" alt="gbdt-1" style="width: 90%;" />


#### General ideas
Initially, the first tree (stump) is created by averaging all the weights. It is used as the first decision tree. The difference between the ground truth and the predicted value is called **residual**.

Next, a decision tree for predicting these residuals are created.

<img src="https://github.com/KarenMars/KarenMars.github.io/blob/main/images/gbdt/20240729171129.png" alt="gbdt-2" style="width: 90%;" />

Afterwards, the values of this tree is added to previous tree with a learning rate. This can explain why they are called gradient boosting decision trees. GBDT trees are a bunch of weak learners. In addition, smaller learning rate in right direction can achieve better results with lower variances.

<img src="https://github.com/KarenMars/KarenMars.github.io/blob/main/images/gbdt/20240729171447.png" alt="gbdt-3" style="width: 90%;" />


#### Regression details
#### How to define the value of leaves

<img src="https://github.com/KarenMars/KarenMars.github.io/blob/main/images/gbdt/gbdt_steps.png" alt="gbdt-4" style="width: 90%;" />

##### Step 1
Firstly, initialize the model with a constant value.

$$F_0(x)={\argmin}_{r}\sum^n_{i=1}L(y_i, \gamma)$$

where $y_i$ represents the observed value, $\gamma$ represents the leaf value. It can be seen that the leaf value is simple the averaged value of all samples.

#### Step 2













